{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95fd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, getpass\n",
    "load_dotenv()  # Loads environment variables from .env file\n",
    "# ignore all warnings in this notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if google_api_key:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "    print(\"GOOGLE_API_KEY found in .env file.\")\n",
    "else:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter GEMINI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf5ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"SSL_NO_VERIFY\"] = \"1\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d02cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Loads environment variables from .env file\n",
    "load_dotenv()  \n",
    "\n",
    "# extracting HUGGINGFACEHUB_API_TOKEN\n",
    "huggingface_api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if huggingface_api_key:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_api_key\n",
    "    print(\"HUGGINGFACEHUB_API_TOKEN found in .env file.\")\n",
    "else:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter HUGGINGFACEHUB API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69138c2",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd1262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762507420.813180   22064 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07559326",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(\"Machine learning is fascinating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240f3e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf42048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0113553237169981,\n",
       " 0.013383835554122925,\n",
       " 0.004231748636811972,\n",
       " -0.09220291674137115,\n",
       " -0.00380918406881392]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49c5538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding 3 sentences...\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"AI and ML are interesting topics\",\n",
    "    \"I love eating pizza\"\n",
    "]\n",
    "\n",
    "print(f\"\\nEmbedding {len(texts)} sentences...\")\n",
    "vectors = [embeddings.embed_query(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eb7e6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cbd1445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0225bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Euclidean Distances:\n",
      "Sentence 1 vs 2 (both about ML): 0.3483\n",
      "Sentence 1 vs 3 (ML vs pizza): 0.6394\n",
      "\n",
      "Dot product:\n",
      "Sentence 1 vs 2 (both about ML): 0.9393\n",
      "Sentence 1 vs 3 (ML vs pizza): 0.7956\n",
      "\n",
      "Cosine Similarities:\n",
      "Sentence 1 vs 2 (both about ML): 0.9393\n",
      "Sentence 1 vs 3 (ML vs pizza): 0.7956\n",
      "Sentence 2 vs 3 (ML vs pizza): 0.7799\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Measures the angle between vectors (0 to 1, higher = more similar)\"\"\"\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "def euclidean_distance(v1, v2):\n",
    "    \"\"\"Measures straight-line distance (lower = more similar)\"\"\"\n",
    "    return norm(np.array(v1) - np.array(v2))\n",
    "def dot_product(v1, v2):\n",
    "    \"\"\"Measures projection (higher = more similar)\"\"\"\n",
    "    return np.dot(v1, v2)\n",
    "\n",
    "print(\"\\nEuclidean Distances:\")\n",
    "print(f\"Sentence 1 vs 2 (both about ML): {euclidean_distance(vectors[0], vectors[1]):.4f}\")\n",
    "print(f\"Sentence 1 vs 3 (ML vs pizza): {euclidean_distance(vectors[0], vectors[2]):.4f}\")\n",
    "print(\"\\nDot product:\")\n",
    "print(f\"Sentence 1 vs 2 (both about ML): {dot_product(vectors[0], vectors[1]):.4f}\")\n",
    "print(f\"Sentence 1 vs 3 (ML vs pizza): {dot_product(vectors[0], vectors[2]):.4f}\")\n",
    "print(\"\\nCosine Similarities:\")\n",
    "print(f\"Sentence 1 vs 2 (both about ML): {cosine_similarity(vectors[0], vectors[1]):.4f}\")\n",
    "print(f\"Sentence 1 vs 3 (ML vs pizza): {cosine_similarity(vectors[0], vectors[2]):.4f}\")\n",
    "print(f\"Sentence 2 vs 3 (ML vs pizza): {cosine_similarity(vectors[1], vectors[2]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee412b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results for: How do neural networks learn?\n",
      "  Score: 0.9283 - Neural networks use backpropagation to adjust weights\n",
      "  Score: 0.8813 - Deep learning models train on large datasets\n",
      "  Score: 0.7709 - The weather forecast for tomorrow\n",
      "  Score: 0.7513 - Best pizza recipes in New York\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# User's search query\n",
    "query = \"How do neural networks learn?\"\n",
    "\n",
    "# Your documents\n",
    "documents = [\n",
    "    \"Neural networks use backpropagation to adjust weights\",\n",
    "    \"Best pizza recipes in New York\",\n",
    "    \"Deep learning models train on large datasets\",\n",
    "    \"The weather forecast for tomorrow\"\n",
    "]\n",
    "\n",
    "# Embed everything\n",
    "query_vector = embeddings.embed_query(query)\n",
    "doc_vectors = [embeddings.embed_query(doc) for doc in documents]\n",
    "\n",
    "# Find most similar documents\n",
    "similarities = [\n",
    "    (doc, cosine_similarity(query_vector, doc_vec))\n",
    "    for doc, doc_vec in zip(documents, doc_vectors)\n",
    "]\n",
    "\n",
    "# Sort by similarity\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nSearch results for:\", query)\n",
    "for doc, score in similarities:\n",
    "    print(f\"  Score: {score:.4f} - {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d143399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- embed_documents (batch processing) ---\n",
      "Embedded 3 documents\n",
      "Each embedding has 3072 dimensions\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- embed_documents (batch processing) ---\")\n",
    "documents = [\n",
    "    \"Machine learning is a subset of AI\",\n",
    "    \"Neural networks are inspired by the brain\",\n",
    "    \"Python is a popular programming language\"\n",
    "]\n",
    "\n",
    "doc_embeddings = embeddings.embed_documents(documents)\n",
    "print(f\"Embedded {len(doc_embeddings)} documents\")\n",
    "print(f\"Each embedding has {len(doc_embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb21d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached Embeddings\n",
    "\n",
    "import time\n",
    "from langchain_classic.embeddings import CacheBackedEmbeddings  \n",
    "from langchain_classic.storage import LocalFileStore \n",
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c806f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = LocalFileStore(\"./cache_/\") \n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    store,\n",
    "    namespace=embeddings.model,\n",
    "    batch_size = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb17a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"First document about machine learning\",\n",
    "    \"Second document about neural networks\",\n",
    "    \"Third document about data science\",\n",
    "] * 10  # 30 documents total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a29ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 30 documents...\n",
      "\n",
      "--- First Run (Cold Cache) ---\n",
      "Time: 1.515 seconds\n",
      "Cache items: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTesting with {len(documents)} documents...\")\n",
    "\n",
    "# First run - no cache\n",
    "print(\"\\n--- First Run (Cold Cache) ---\")\n",
    "start = time.time()\n",
    "embeddings_1 = cached_embedder.embed_documents(documents)\n",
    "time_1 = time.time() - start\n",
    "print(f\"Time: {time_1:.3f} seconds\")\n",
    "print(f\"Cache items: {len(list(store.yield_keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Second Run (Warm Cache) ---\n",
      "Time: 0.035 seconds\n",
      "Speedup: 43.4x faster!\n",
      "\n",
      "Embeddings match: True\n"
     ]
    }
   ],
   "source": [
    "# Second run - with cache\n",
    "print(\"\\n--- Second Run (Warm Cache) ---\")\n",
    "start = time.time()\n",
    "embeddings_2 = cached_embedder.embed_documents(documents)\n",
    "time_2 = time.time() - start\n",
    "print(f\"Time: {time_2:.3f} seconds\")\n",
    "print(f\"Speedup: {time_1/time_2:.1f}x faster!\")\n",
    "\n",
    "# Verify they're identical\n",
    "print(f\"\\nEmbeddings match: {np.allclose(embeddings_1[0], embeddings_2[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "572586a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: 'Machine learning is fascinating'\n",
      "Cache key (SHA-256 hash): d8b46f29f642efbe0a2696d8ee10037a9be70689848b2c9f5427fbd9dfa182fe\n",
      "\n",
      "Key insight: Same text → Same hash → Cache hit\n",
      "\n",
      "Total cached keys: 3\n",
      "Example cache keys:\n",
      "  - models/gemini-embedding-001f45b0927-1dc1-5571-bf7d-6b51b178975c\n",
      "  - models/gemini-embedding-0018f190f63-952e-5284-a87a-4e175f161d45\n",
      "  - models/gemini-embedding-001fb2c97be-3aa7-5ff8-90de-a6f1bdb706d8\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "text = \"Machine learning is fascinating\"\n",
    "cache_key = hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "print(f\"\\nOriginal text: '{text}'\")\n",
    "print(f\"Cache key (SHA-256 hash): {cache_key}\")\n",
    "print(f\"\\nKey insight: Same text → Same hash → Cache hit\")\n",
    "\n",
    "# Show cached keys\n",
    "keys = list(store.yield_keys())\n",
    "print(f\"\\nTotal cached keys: {len(keys)}\")\n",
    "if keys:\n",
    "    print(f\"Example cache keys:\")\n",
    "    for key in list(keys)[:3]:\n",
    "        print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07fd0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache both documents and queries\n",
    "fully_cached = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    store,\n",
    "    query_embedding_cache=True,  # Reuse same store for queries\n",
    "    namespace=\"gemini-all\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use separate stores\n",
    "query_store = LocalFileStore(\"./query_cache/\")\n",
    "separate_cached = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    store,\n",
    "    query_embedding_cache=query_store,  # Separate cache for queries\n",
    "    namespace=\"gemini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating a RAG application...\n",
      "\n",
      "Embedding 3 documents (will be cached)...\n",
      "Document embeddings cached: 3\n",
      "\n",
      "Processing user queries...\n",
      "1. 'What is LangChain?' (NEW QUERY) - 0.3307s\n",
      "2. 'Tell me about vector databases' (NEW QUERY) - 0.3106s\n",
      "3. 'What is LangChain?' (CACHE HIT) - 0.0013s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Simulating a RAG application...\")\n",
    "\n",
    "# Embed documents once (cached)\n",
    "docs = [\n",
    "    \"LangChain is a framework for building LLM applications\",\n",
    "    \"Vector databases store embeddings for semantic search\",\n",
    "    \"RAG combines retrieval and generation for better answers\"\n",
    "]\n",
    "\n",
    "print(f\"\\nEmbedding {len(docs)} documents (will be cached)...\")\n",
    "doc_vecs = fully_cached.embed_documents(docs)\n",
    "print(f\"Document embeddings cached: {len(doc_vecs)}\")\n",
    "\n",
    "# User queries (these get cached too if repeated)\n",
    "queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"Tell me about vector databases\",\n",
    "    \"What is LangChain?\"  # Repeated query - should hit cache\n",
    "]\n",
    "\n",
    "print(\"\\nProcessing user queries...\")\n",
    "for i, query in enumerate(queries, 1):\n",
    "    start = time.time()\n",
    "    query_vec = fully_cached.embed_query(query)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    cache_status = \"CACHE HIT\" if i == 3 else \"NEW QUERY\"\n",
    "    print(f\"{i}. '{query}' ({cache_status}) - {elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32202bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adba5bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total cached embeddings: 8\n",
      "Cache cleared!\n"
     ]
    }
   ],
   "source": [
    "# Check cache stats\n",
    "print(f\"\\nTotal cached embeddings: {len(list(store.yield_keys()))}\")\n",
    "# Clear cache if needed\n",
    "store.mdelete(list(store.yield_keys()))\n",
    "print(\"Cache cleared!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5b438",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "331ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine learning is a subset of AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cf9d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1de04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02326645515859127, -0.056493259966373444, -0.009426760487258434, -0.02003607153892517, 0.0504939\n"
     ]
    }
   ],
   "source": [
    "single_vector = embeddings.embed_query(text)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0e30fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02327197976410389, -0.05646820738911629, -0.009404956363141537, -0.020021595060825348, 0.0505444\n",
      "[-0.03336711227893829, 0.01044490933418274, 0.011859860271215439, -0.04046759381890297, 0.0021111709\n"
     ]
    }
   ],
   "source": [
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600233a",
   "metadata": {},
   "source": [
    "## HugginFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40ae0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bb960fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04870401695370674, -0.01661957986652851, 0.06689752638339996]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e9ca6",
   "metadata": {},
   "source": [
    "### Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7437840",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU  langchain langchain-huggingface sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a671701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c72f51",
   "metadata": {},
   "source": [
    "### Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use separate stores\n",
    "query_store = LocalFileStore(\"./query_cache/\")\n",
    "separate_cached = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embedder=embeddings,\n",
    "    document_embedding_cache=store,\n",
    "    query_embedding_cache=query_store,  # Separate cache for queries\n",
    "    namespace=\"openai\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d622e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
